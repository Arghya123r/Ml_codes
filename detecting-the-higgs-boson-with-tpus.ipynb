{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Searching for the Higgs Boson #\n\nThe Standard Model is a theory in particle physics that describes some of the most basic forces of nature. One fundamental particle, the Higgs boson, is what accounts for the *mass* of matter. First theorized in the 1964, the Higgs boson eluded observation for almost fifty years. In 2012 it was finally observed experimentally at the Large Hadron Collider. These experiments produced millions of gigabytes of data.\n\nLarge and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n\n# The Collision Data #\n\nThe collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead observe the behavior of the particles they decay into, their \"decay products\".\n\nThe *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.\n\n# Wide and Deep Neural Networks #\n\nA *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n\nBoth the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https://www.kaggle.com/docs/tpu) (TPUs), an accelerator ideal for large workloads.\n\nWe've collected some hyperparameters here to make experimentation easier. Fork this notebook by [**clicking here**](https://www.kaggle.com/kernels/fork/12171965) to try it yourself!","metadata":{}},{"cell_type":"code","source":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:35:51.956916Z","iopub.execute_input":"2023-09-09T13:35:51.957887Z","iopub.status.idle":"2023-09-09T13:35:51.977187Z","shell.execute_reply.started":"2023-09-09T13:35:51.957816Z","shell.execute_reply":"2023-09-09T13:35:51.976164Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n\n# Setup #\n\nIn addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them.","metadata":{}},{"cell_type":"code","source":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\n# Detect and init the TPU\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:36:15.331237Z","iopub.execute_input":"2023-09-09T13:36:15.331636Z","iopub.status.idle":"2023-09-09T13:37:05.178446Z","shell.execute_reply.started":"2023-09-09T13:36:15.331606Z","shell.execute_reply":"2023-09-09T13:37:05.177328Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"D0909 13:36:47.081936856      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0909 13:36:47.081963294      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0909 13:36:47.081967087      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0909 13:36:47.081970059      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0909 13:36:47.081972895      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0909 13:36:47.081975775      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0909 13:36:47.081978491      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0909 13:36:47.081981152      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0909 13:36:47.081983886      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0909 13:36:47.081986697      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0909 13:36:47.081989335      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0909 13:36:47.081991957      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0909 13:36:47.081994600      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0909 13:36:47.081997130      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0909 13:36:47.082207169      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD0909 13:36:47.082218569      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0909 13:36:47.082268095      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0909 13:36:47.090200615      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0909 13:36:47.090214187      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0909 13:36:47.090218158      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0909 13:36:47.090221492      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0909 13:36:47.090224765      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0909 13:36:47.090228158      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0909 13:36:47.090235992      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0909 13:36:47.090254181      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0909 13:36:47.090285022      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0909 13:36:47.090302160      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0909 13:36:47.090306123      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0909 13:36:47.090310004      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0909 13:36:47.090317051      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0909 13:36:47.090320647      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0909 13:36:47.090324284      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0909 13:36:47.090328609      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0909 13:36:47.092675246      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0909 13:36:47.125666665     242 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0909 13:36:47.198346065      15 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-09-09T13:36:47.198324515+00:00\"}\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version 2.12.0\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nNumber of accelerators:  8\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_15/23444675.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n\n# Load Data #\n\nThe dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training.","metadata":{}},{"cell_type":"code","source":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:37:25.735302Z","iopub.execute_input":"2023-09-09T13:37:25.735701Z","iopub.status.idle":"2023-09-09T13:37:25.744585Z","shell.execute_reply.started":"2023-09-09T13:37:25.735675Z","shell.execute_reply":"2023-09-09T13:37:25.743457Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size // batch_size\nvalidation_steps = validation_size // batch_size\n\n# For model.compile\nsteps_per_execution = 256","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:37:31.086106Z","iopub.execute_input":"2023-09-09T13:37:31.086487Z","iopub.status.idle":"2023-09-09T13:37:31.092277Z","shell.execute_reply.started":"2023-09-09T13:37:31.086451Z","shell.execute_reply":"2023-09-09T13:37:31.091377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:37:34.496392Z","iopub.execute_input":"2023-09-09T13:37:34.496792Z","iopub.status.idle":"2023-09-09T13:37:34.697457Z","shell.execute_reply.started":"2023-09-09T13:37:34.496747Z","shell.execute_reply":"2023-09-09T13:37:34.696322Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"get_gcs_path is not required on TPU VMs which can directly use Kaggle datasets, using path: /kaggle/input/higgs-boson\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model #\n\nNow that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n","metadata":{}},{"cell_type":"code","source":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\nwith strategy.scope():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[28])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    outputs = layers.Dense(1)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='sigmoid',\n    )\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n    steps_per_execution=steps_per_execution,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:38:22.400879Z","iopub.execute_input":"2023-09-09T13:38:22.401256Z","iopub.status.idle":"2023-09-09T13:38:23.156193Z","shell.execute_reply.started":"2023-09-09T13:38:22.401228Z","shell.execute_reply":"2023-09-09T13:38:23.155202Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training #\n\nDuring training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch.","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-09-09T13:38:27.321691Z","iopub.execute_input":"2023-09-09T13:38:27.322137Z","iopub.status.idle":"2023-09-09T13:38:27.328088Z","shell.execute_reply.started":"2023-09-09T13:38:27.322103Z","shell.execute_reply":"2023-09-09T13:38:27.327046Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"history = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[early_stopping, lr_schedule],\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:38:31.601537Z","iopub.execute_input":"2023-09-09T13:38:31.602293Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n640/640 [==============================] - 318s 497ms/step - loss: 0.5685 - auc: 0.7844 - binary_accuracy: 0.7111 - val_loss: 0.4988 - val_auc: 0.8337 - val_binary_accuracy: 0.7514 - lr: 0.0010\nEpoch 2/50\n256/640 [===========>..................] - ETA: 16s - loss: 0.4968 - auc: 0.8353 - binary_accuracy: 0.7529","output_type":"stream"}]},{"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References #\n\n- Baldi, P. et al. *Searching for Exotic Particles in High-Energy Physics with Deep Learning*. (2014) ([arXiv](https://arxiv.org/abs/1402.4735))\n- Cheng, H. et al. *Wide & Deep Learning for Recommender Systems*. (2016) ([arXiv](https://arxiv.org/abs/1606.07792))\n- *What Exactly is the Higgs Boson?* Scientific American. (1999) [(article)](https://www.scientificamerican.com/article/what-exactly-is-the-higgs/)]\n\n[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/191966) to chat with other Learners.*","metadata":{}}]}